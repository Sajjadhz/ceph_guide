Vagrant.configure("2") do |config|
  # Define variables
  base_ip = "192.168.56."
  ip_addresses = { vm1: "#{base_ip}7", vm2: "#{base_ip}8", vm3: "#{base_ip}9" }
  ssh_username = "root"
  ssh_password = "admin"
  dashboard_username = "admin"
  dashboard_password = "admin"

  (1..3).each do |i|
    config.vm.define "vm#{i}" do |vm_config|
      vm_config.vm.box = "sysnet4admin/Rocky8"
      vm_config.vm.box_version = "8.3"
      vm_config.vm.hostname = "vm#{i}"
      
      # Network Configuration
      vm_config.vm.network "private_network", ip: ip_addresses[:"vm#{i}"]

      # SSH Configuration
      vm_config.ssh.username = ssh_username
      vm_config.ssh.password = ssh_password
      vm_config.ssh.insert_key = false
      vm_config.ssh.keys_only = false

      # Provider Configuration
      vm_config.vm.provider "virtualbox" do |vb|
        vb.memory = "2048"
        vb.cpus = 2
        vb.name = "vm#{i}"

        # Get disk path for current VM
        vb_machine_folder = `VBoxManage list systemproperties | grep "Default machine folder" | cut -d ':' -f2`.strip
        (1..3).each do |disk_index|
          disk_path = File.join(vb_machine_folder, "vm#{i}", "disk#{i}_#{disk_index}.vdi")

          # Create disk if it does not exist
          unless File.exist?(disk_path)
            vb.customize ["createhd", "--filename", disk_path, "--format", "VDI", "--size", 20480] # 20 GB
          end

          # Attach the disk
          vb.customize ["storageattach", :id, "--storagectl", "SATA", "--port", "#{disk_index}", "--device", "0", "--type", "hdd", "--medium", disk_path]
        end

        # Ensure primary disk is attached and bootable
        vb.customize ["modifyvm", :id, "--boot1", "disk"]
      end

      # Provisioning for all VMs
      vm_config.vm.provision "shell", inline: <<-SHELL
        # Enable ssh password authentication
        sed -i 's/^PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
        sed -i 's/.*PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
        systemctl reload sshd

        # Set Root password
        echo -e "#{ssh_password}\n#{ssh_password}" | passwd #{ssh_username} >/dev/null 2>&1

        # Install and configure prerequisites
        sudo dnf update -y
        sudo dnf install -y chrony lvm2 podman python3.9 sshpass
        sudo systemctl enable --now chronyd

        # Disable firewall
        sudo systemctl stop firewalld.service
        sudo systemctl disable firewalld.service

        # Configure time synchronization
        echo "allow 192.168.0.0/16" | sudo tee -a /etc/chrony.conf
        echo "server #{ip_addresses[:vm1]} iburst" | sudo tee -a /etc/chrony.conf
        sudo systemctl restart chronyd
        sudo chronyc sources
        sudo timedatectl set-timezone Asia/Tehran
        sudo timedatectl status
      SHELL
    end
  end

  # Additional provisioning for vm3 (install and configure Ceph)
  config.vm.define "vm3" do |vm3_config|
    vm3_config.vm.provision "shell", inline: <<-SHELL
      # Install Cephadm
      CEPH_RELEASE=18.2.4
      curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm
      chmod +x ./cephadm
      sudo ./cephadm add-repo --release quincy
      sudo ./cephadm install

      # Bootstrap the Ceph cluster
      sudo ./cephadm bootstrap --mon-ip #{ip_addresses[:vm3]}

      KEY_NAME="id_rsa"
      KEY_PATH="$HOME/.ssh/$KEY_NAME"
      PASSWORD="#{ssh_password}"
      HOSTS=("#{ip_addresses[:vm1]}" "#{ip_addresses[:vm2]}" "#{ip_addresses[:vm3]}")

      # Generate SSH key without prompting (no passphrase)
      ssh-keygen -t rsa -b 4096 -f $KEY_PATH -N "" -q

      # Copy the key to each host
      for HOST in "${HOSTS[@]}"; do
          sshpass -p "$PASSWORD" ssh-copy-id -i "$KEY_PATH.pub" -o StrictHostKeyChecking=no root@$HOST
      done

      # Copy the Ceph public key to other nodes
      sudo ./cephadm shell -- ceph cephadm get-pub-key > ~/ceph.pub
      sshpass -p '#{ssh_password}' ssh-copy-id -o StrictHostKeyChecking=no -f -i ~/ceph.pub root@#{ip_addresses[:vm1]}
      sshpass -p '#{ssh_password}' ssh-copy-id -o StrictHostKeyChecking=no -f -i ~/ceph.pub root@#{ip_addresses[:vm2]}
      sshpass -p '#{ssh_password}' ssh-copy-id -o StrictHostKeyChecking=no -f -i ~/ceph.pub root@#{ip_addresses[:vm3]}

      # Add the other nodes to the Ceph cluster
      sudo ./cephadm shell -- ceph orch host add vm1 #{ip_addresses[:vm1]}
      sudo ./cephadm shell -- ceph orch host add vm2 #{ip_addresses[:vm2]}

      # Deploy Monitor Daemons
      sudo ./cephadm shell -- ceph orch apply mon --placement="vm1,vm2,vm3"
      # Deploy Manager Daemons
      sudo ./cephadm shell -- ceph orch apply mgr --placement="vm1,vm2,vm3"

      # Deploy OSDs on each node
      sudo ./cephadm shell -- ceph orch daemon add osd vm1:/dev/sdb
      sudo ./cephadm shell -- ceph orch daemon add osd vm1:/dev/sdc
      sudo ./cephadm shell -- ceph orch daemon add osd vm1:/dev/sdd
      sudo ./cephadm shell -- ceph orch daemon add osd vm2:/dev/sdb
      sudo ./cephadm shell -- ceph orch daemon add osd vm2:/dev/sdc
      sudo ./cephadm shell -- ceph orch daemon add osd vm2:/dev/sdd
      sudo ./cephadm shell -- ceph orch daemon add osd vm3:/dev/sdb
      sudo ./cephadm shell -- ceph orch daemon add osd vm3:/dev/sdc
      sudo ./cephadm shell -- ceph orch daemon add osd vm3:/dev/sdd

      # Set dashboard admin username and password
      sudo ./cephadm shell --  ceph dashboard create-self-signed-cert
      echo -n "#{dashboard_password}" > /tmp/dashboard-password
      sudo ./cephadm shell -- ceph dashboard set-login-credentials #{dashboard_username} -i /tmp/dashboard-password
      sudo rm /tmp/dashboard-password

      # Verify the Ceph cluster status
      sudo ./cephadm shell -- sudo ceph mgr services
      sudo ./cephadm shell -- ceph -v
      sudo ./cephadm shell -- ceph -s
    SHELL
  end
end
